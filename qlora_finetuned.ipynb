{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# path_codex=\"/media/data/flowers/OpenELM/Codex_PAPER_1M_iter_0.txt\"\n",
    "# # with open(path_codex, 'r') as f:\n",
    "# #     codex = f.read()\n",
    "#     # codex = [x.strip() for x in codex]\n",
    "# f = open(path_codex, 'r')\n",
    "# out=f.read()\n",
    "# f.close()\n",
    "\n",
    "# path_save=\"/media/data/flowers/OpenELM/codex_dataset.json\"\n",
    "# def format_dataset(puzzles):\n",
    "#     puzzles_prompt=\"\"\n",
    "#     for i in range(len(puzzles)):\n",
    "#         puzzles_prompt += f\"Puzzle {i}:\\n\"+puzzles[i]+\"\\n\"\n",
    "\n",
    "#     return puzzles_prompt\n",
    "# n_fewshot=4\n",
    "# dataset =[]\n",
    "# split_out=out.split(\"\\nassert f(g())\\n\\n\")\n",
    "# split_out = [x+\"\\nassert f(g())\\n\" for x in split_out if x]\n",
    "# split_out = split_out[:-1]\n",
    "# for idx in range(0,len(split_out),n_fewshot):\n",
    "#     split_out_n_fs = split_out[idx:idx+n_fewshot]\n",
    "#     if len(split_out_n_fs)==n_fewshot:\n",
    "#         dataset.append(format_dataset(split_out_n_fs))\n",
    "# dataset_json= [{'text': puzz} for puzz in dataset]\n",
    "# with open(path_save, 'w') as outfile:\n",
    "#     json.dump(dataset_json, outfile,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /projets/flowers/julien/miniconda3/envs/llm did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('3128'), PosixPath('http'), PosixPath('//proxy.plafrim.cluster')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/jpourcel/.guix-profile/lib/locale')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//guix.plafrim.cluster'), PosixPath('guix')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval `/usr/bin/modulecmd bash $*`\\n}')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/projets/flowers/julien/hf/datasets')}\n",
      "  warn(msg)\n",
      "/projets/flowers/julien/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = \"/projets/flowers/julien/hf/datasets\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/projets/flowers/julien/models/\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,TrainingArguments\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# from trl import SFTTrainer\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train on puzzle generated (except P3 trainset)\n",
    "#https://huggingface.co/docs/trl/main/en/sft_trainer\n",
    "\n",
    "\n",
    "# testset which is in fact the trainset from P3 dataset\n",
    "# path_test = \"/media/data/flowers/OpenELM/preprocess_p3.json\"\n",
    "\n",
    " \n",
    "# /!\\ need to remove that, just for testing purpose /!\\\n",
    "\n",
    "# path_train = path_test \n",
    "# ================================================\n",
    "\n",
    "# test_set = load_dataset(\"json\", data_files=path_test)\n",
    "model_id = \"Salesforce/codegen-350M-mono\"#\"bigcode/starcoderbase-1b\"  \"Salesforce/codegen-350M-mono\" # mono is better\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"#torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config, device_map=\"auto\",trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"qkv_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_func(example):\n",
    "#     # text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "#     text = example[\"text\"]\n",
    "#     return text\n",
    "path_train= \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # put path here\n",
    "path_save=\"/media/data/flowers/OpenELM/codex_dataset.json\"\n",
    "dataset = load_dataset(\"json\", data_files=path_save,split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.001)\n",
    "\n",
    "training_arguments=TrainingArguments(\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    do_eval=True,\n",
    "    # warmup_steps=2,\n",
    "    warmup_ratio=0.01,\n",
    "    max_steps = 2000,\n",
    "    eval_steps=10,\n",
    "    weight_decay=0.001,\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    # fp16=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"adamw_hf\",#\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,#\"EleutherAI/gpt-neo-125m\",\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    args= training_arguments\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/media/data/flowers/OpenELM/models/codegen-QDAIF-smart\"\n",
    "trainer.save_model(output_dir)\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "del model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test_puzzle(test_fg):\n",
    "    test_fg= \"from typing import *\\n\"+test_fg\n",
    "    try:\n",
    "        exec(test_fg)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "prompt_solve_puzzle='''You will be given a function and its docstring. Respond only in code with a correct, efficient implementation of the function.\n",
    "You need to generate the correct solutions (g), for the Problem 3 that satisfies the condition f(g()) == True.\n",
    "Problem 0:\n",
    "```\n",
    "def f(stamps: List[int], target=80, max_stamps=4, options=[10, 32, 8]) -> bool:\n",
    "    \"\"\"Find a selection of at most max_stamps stamps whose total worth is the target value.\"\"\"\n",
    "    for s in stamps:\n",
    "        assert s in options\n",
    "    return len(stamps) <= max_stamps and sum(stamps) == target\n",
    "```\n",
    "Solution 0:\n",
    "```\n",
    "def g(target = 80, max_stamps = 4, options = [10, 32, 8]):\n",
    "    from itertools import combinations_with_replacement\n",
    "    for n in range(max_stamps + 1):\n",
    "        for c in combinations_with_replacement(options, n):\n",
    "            if sum(c) == target:\n",
    "                return list(c)\n",
    "assert f(g())\n",
    "```\n",
    "Problem 1:\n",
    "```\n",
    "from typing import*\n",
    "def f(ans: List[List[int]], target=2) -> bool:\n",
    "    \"\"\"\n",
    "    Find a list of pairs of integers where the number of pairs in which the second number is more than\n",
    "    two greater than the first number is a given constant\n",
    "    \"\"\"\n",
    "    for i in range(len(ans)):\n",
    "        a, b = ans[i]\n",
    "        if b - a >= 2:\n",
    "            target -= 1\n",
    "    return target == 0\n",
    "```\n",
    "Solution 1:\n",
    "```\n",
    "def g(target = 2):\n",
    "    return [[0, 2]] * target \n",
    "assert f(g()) == True\n",
    "```\n",
    "Problem 2:\n",
    "```\n",
    "def f(n: int, v=313946483, w=806690290) -> bool:\n",
    "    \"\"\"Find the smallest n such that if v is tripled n times and w is doubled n times, v exceeds w.\"\"\"\n",
    "    for i in range(n):\n",
    "        assert v <= w\n",
    "        v *= 3\n",
    "        w *= 2\n",
    "    return v > w\n",
    "```\n",
    "Solution 2:\n",
    "```\n",
    "def g(v = 313946483, w = 806690290):\n",
    "    i = 0\n",
    "    while v <= w:\n",
    "        v *= 3\n",
    "        w *= 2\n",
    "        i += 1\n",
    "    return i \n",
    "assert f(g()) == True\n",
    "```\n",
    "Problem 3:\n",
    "```\n",
    "{pb}\n",
    "```\n",
    "Solution 3:\n",
    "```\n",
    "'''\n",
    "\n",
    "\n",
    "def pass_at_k(n, c, k):\n",
    "    \"\"\"\n",
    "    Adapted from \"Evaluating Large Language Models Trained on Code\" (https://arxiv.org/abs/2107.03374)\n",
    "\n",
    "    :param n: total number of samples\n",
    "    :param c: number of correct samples\n",
    "    :param k: k in pass@k\n",
    "    \"\"\"\n",
    "    assert n >= k\n",
    "    if n - c < k:\n",
    "        return 1.0\n",
    "    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b177d04bcf491690dd38ce13ab9171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id=\"codellama/CodeLlama-7b-Python-hf\"\n",
    "# del model\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side='left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "# model.config.use_cache = True\n",
    "model=torch.compile(model)\n",
    "# prompt = 'def remove_non_ascii(s: str) -> str:\\n\"\"\" '\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# output = model.generate(\n",
    "#     inputs[\"input_ids\"],\n",
    "#     max_new_tokens=64,\n",
    "#     do_sample=True,\n",
    "#     top_p=0.9,\n",
    "#     temperature=0.1,\n",
    "# )\n",
    "\n",
    "# output = output[0].to(\"cpu\")\n",
    "# print(tokenizer.decode(output))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "path_trainset=\"/projets/flowers/julien/OpenELM/src/openelm/utils/preprocess_p3_emb.json\"\n",
    "with open(path_trainset, 'r') as f:\n",
    "    trainset = json.load(f)\n",
    "    \n",
    "list_trainset= [x[\"program_str\"] for x in trainset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.suppress_errors = True\n",
    "list_puzzle_correct=[]\n",
    "correct_puzz=0\n",
    "curr_idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/536 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============ idx 0 ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/536 [00:11<1:43:15, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "\n",
      "def f(s: str) -> bool:\n",
      "    \"\"\"Find a string with 1000 'o's but no two adjacent 'o's.\"\"\"\n",
      "    return s.count('o') == 1000 and s.count('oo') == 0\n",
      "def g(s = \"o\"*1000):\n",
      "    return s\n",
      "\n",
      "assert f(g()) == True\n",
      "\n",
      "correct puzzles: 0/1\n",
      "\n",
      "\n",
      "============ idx 1 ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/536 [00:23<1:42:46, 11.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "\n",
      "def f(li: List[int]) -> bool:\n",
      "    \"\"\"Find a permutation of [0, 1, ..., 998] such that the ith element is *not* i, for all i=0, 1, ..., 998.\"\"\"\n",
      "    return sorted(li) == list(range(999)) and all((li[i] != i for i in range(len(li))))\n",
      "def g(n = 999):\n",
      "    return sorted(list(range(n)), reverse=True)\n",
      "\n",
      "assert f(g()) == True\n",
      "\n",
      "correct puzzles: 0/2\n",
      "\n",
      "\n",
      "============ idx 2 ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/536 [00:34<1:41:54, 11.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "\n",
      "def f(li: List[int]) -> bool:\n",
      "    \"\"\"Find a list integers such that the integer i occurs i times, for i = 0, 1, 2, ..., 9.\"\"\"\n",
      "    return all([li.count(i) == i for i in range(10)])\n",
      "def g():\n",
      "    return [0] * 10\n",
      "\n",
      "assert f(g()) == True\n",
      "\n",
      "correct puzzles: 0/3\n",
      "\n",
      "\n",
      "============ idx 3 ==================\n",
      "\n",
      "\n",
      "-------------------\n",
      "\n",
      "def f(s: str) -> bool:\n",
      "    \"\"\"Find a three-digit pattern  that occurs more than 8 times in the decimal representation of 8^2888.\"\"\"\n",
      "    return str(8 ** 2888).count(s) > 8 and len(s) == 3\n",
      "def g():\n",
      "    s = str(8 ** 2888)    \n",
      "    return s[0], s[1], s[2]\n",
      "\n",
      "assert f(g()) == True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/536 [04:01<13:07:53, 88.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct puzzles: 0/4\n",
      "\n",
      "\n",
      "============ idx 4 ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/536 [04:13<8:59:39, 60.98s/it] Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "\n",
      "def f(li: List[int]) -> bool:\n",
      "    \"\"\"\n",
      "    Find a way to rearrange the letters in the pangram \"The quick brown fox jumps over the lazy dog\" to get\n",
      "    the pangram \"The five boxing wizards jump quickly\". The answer should be represented as a list of index\n",
      "    mappings.\n",
      "    \"\"\"\n",
      "    return ['The quick brown fox jumps over the lazy dog'[i] for i in li] == list('The five boxing wizards jump quickly')\n",
      "def g():\n",
      "    li = [17, 27, 12, 10, 5, 13, 1, 3, 15, 8, 16, 21, 20, 11, 6, 18, 19, 25, 4, 2, 22, 14, 23, 9, 14, 7, 18, 24, 13]\n",
      "    return li \n",
      "\n",
      "assert f(g()) == True\n",
      "\n",
      "correct puzzles: 0/5\n",
      "\n",
      "\n",
      "============ idx 5 ==================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences=10 #n_try\n",
    "list_passk=[]\n",
    "list_puzzle=[]\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx in tqdm(range(curr_idx,len(list_trainset))): #len(dataset[\"test\"])\n",
    "        curr_idx=idx\n",
    "        # idx=0\n",
    "        print(f\"\\n\\n============ idx {idx} ==================\\n\")\n",
    "        flag=True\n",
    "        attempt=0\n",
    "        list_puzzle_idx=[]\n",
    "        while flag and attempt<50:\n",
    "            attempt+=1\n",
    "            try:\n",
    "                puzzle= list_trainset[idx]\n",
    "                prompt_f = puzzle.split(\"def g(\")[0]\n",
    "                prompt = prompt_solve_puzzle.format(pb=prompt_f)\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=256,do_sample=True, temperature=0.8,num_return_sequences=num_return_sequences)\n",
    "                len_prompt=inputs[\"input_ids\"][0].shape[0]\n",
    "                flag=False\n",
    "                cor_puz=0\n",
    "                \n",
    "                for i in range(len(outputs)):\n",
    "                    \n",
    "                    # out =\"def g(\"\n",
    "                    out = tokenizer.decode(outputs[i][len_prompt:], skip_special_tokens=True)\n",
    "                    # print(\"output gene[rated:\",out)\n",
    "                    \n",
    "                    extract_g=out.split(\"```\")[0].split(\"assert\")[0]\n",
    "                    extract_g = extract_g+\"\\nassert f(g()) == True\\n\"\n",
    "                    test_fg= prompt_f+extract_g \n",
    "                    list_puzzle.append(test_fg)\n",
    "                    if i<1:\n",
    "                        print(\"\\n-------------------\\n\")\n",
    "                        print(test_fg)\n",
    "                    if test_puzzle(test_fg):\n",
    "                        if cor_puz==0:\n",
    "                            print(\"\\n-------------------\\n\")\n",
    "                            print(test_fg)\n",
    "                        cor_puz+=1\n",
    "\n",
    "                        list_puzzle_idx.append(test_fg)\n",
    "                    n_sample, n_correct=10,cor_puz\n",
    "                    pass_k = pass_at_k(n_sample, n_correct, k=10)\n",
    "            except Exception as e:\n",
    "                print(\"pb when generating puzzles\")\n",
    "                print(str(e))\n",
    "                pass_k=0.\n",
    "                pass\n",
    "\n",
    "\n",
    "        list_puzzle_correct.append(list_puzzle_idx)   \n",
    "\n",
    "        if cor_puz>=1:\n",
    "            print(\"\\n=================\\n\")\n",
    "            print(\"correct puzzle\",correct_puzz)\n",
    "            correct_puzz+=1\n",
    "        list_passk.append(pass_k)\n",
    "\n",
    "        print(f\"correct puzzles: {correct_puzz}/{idx+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def f(s: str) -> bool:\n",
      "    \"\"\"Find a string with 1000 'o's but no two adjacent 'o's.\"\"\"\n",
      "    return s.count('o') == 1000 and s.count('oo') == 0\n",
      "def g():\n",
      "    return ('h' + 'o') * 1000\n",
      "assert f(g())\n"
     ]
    }
   ],
   "source": [
    "print(list_trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.config.suppress_errors = True\n",
    "with torch.no_grad():\n",
    "    idx=0\n",
    "    puzzle= list_trainset[idx:idx+4]\n",
    "    input_tok=tokenizer(puzzle, return_tensors='pt', padding=True).to(\"cuda\")\n",
    "    out_model = model.generate(**input_tok, max_new_tokens=256,do_sample=True, temperature=0.5)\n",
    "    outokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_tok[\"input_ids\"])\n",
    "input_tok[\"input_ids\"][0]\n",
    "out = tokenizer.decode(out_model[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def f(s: str) -> bool:\\n    \"\"\"Find a string with 1000 \\'o\\'s but no two adjacent \\'o\\'s.\"\"\"\\n    return s.count(\\'o\\') == 1000 and s.count(\\'oo\\') == 0\\ndef g():\\n    return (\\'h\\' + \\'o\\') * 1000\\nassert f(g())\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASS 4\n",
    "\n",
    "b=4\n",
    "num_return_sequences=10 #n_try\n",
    "list_passk=[]\n",
    "list_puzzle=[]\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx in tqdm(range(0,curr_idx,len(list_trainset)),bs): #len(dataset[\"test\"])\n",
    "        curr_idx=idx\n",
    "        # idx=0\n",
    "        print(f\"\\n\\n============ idx {idx} ==================\\n\")\n",
    "        flag=True\n",
    "        attempt=0\n",
    "        list_puzzle_idx=[]\n",
    "        while flag and attempt<10:\n",
    "            attempt+=1\n",
    "            try:\n",
    "                puzzle= list_trainset[idx]\n",
    "                prompt_f = puzzle.split(\"def g(\")[0]\n",
    "                prompt = prompt_solve_puzzle.format(pb=prompt_f)\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=256,do_sample=True, temperature=0.5)\n",
    "                len_prompt=inputs[\"input_ids\"][0].shape[0]\n",
    "                flag=False\n",
    "                cor_puz=0\n",
    "                \n",
    "                for i in range(len(outputs)):\n",
    "                    \n",
    "                    # out =\"def g(\"\n",
    "                    out = tokenizer.decode(outputs[i][len_prompt:], skip_special_tokens=True)\n",
    "                    # print(\"output gene[rated:\",out)\n",
    "                    \n",
    "                    extract_g=out.split(\"```\")[0].split(\"assert\")[0]\n",
    "                    extract_g = extract_g+\"\\nassert f(g()) == True\\n\"\n",
    "                    test_fg= prompt_f+extract_g \n",
    "                    list_puzzle.append(test_fg)\n",
    "                    if i<1:\n",
    "                        print(\"\\n-------------------\\n\")\n",
    "                        print(test_fg)\n",
    "                    if test_puzzle(test_fg):\n",
    "                        if cor_puz==0:\n",
    "                            print(\"\\n-------------------\\n\")\n",
    "                            print(test_fg)\n",
    "                        cor_puz+=1\n",
    "\n",
    "                        list_puzzle_idx.append(test_fg)\n",
    "                    n_sample, n_correct=10,cor_puz\n",
    "                    pass_k = pass_at_k(n_sample, n_correct, k=10)\n",
    "            except Exception as e:\n",
    "                print(\"pb when generating puzzles\")\n",
    "                print(str(e))\n",
    "                pass_k=0.\n",
    "                pass\n",
    "\n",
    "\n",
    "        list_puzzle_correct.append(list_puzzle_idx)   \n",
    "\n",
    "        if cor_puz>=1:\n",
    "            print(\"\\n=================\\n\")\n",
    "            print(\"correct puzzle\",correct_puzz)\n",
    "            correct_puzz+=1\n",
    "        list_passk.append(pass_k)\n",
    "\n",
    "        print(f\"correct puzzles: {correct_puzz}/{idx+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train only on completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/media/data/flowers/OpenELM/models\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# train on puzzle generated (except P3 trainset)\n",
    "#https://huggingface.co/docs/trl/main/en/sft_trainer\n",
    "\n",
    "\n",
    "path_train= \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # put path here\n",
    "dataset = load_dataset(\"json\", data_files=path_train, split=\"train\")\n",
    "\n",
    "\n",
    "model_id = \"Salesforce/codegen2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"#torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config, device_map=\"auto\",trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "path = \"/media/data/flowers/OpenELM/preprocess_p3.json\" \n",
    "\n",
    "# path = \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # QDAIF smart\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "f=data[0][\"program_str\"].split(\"def g\")[0]\n",
    "prompt=\"\"\n",
    "n_fewshot=3\n",
    "for i in range(1,1+n_fewshot):\n",
    "    prompt+=f\"Puzzle {i}:\\n\"+data[i][\"program_str\"]+\"\\n\\n\"\n",
    "# text = prompt+ f\"Puzzle {n_fewshot}:\\n\"+ f +\"\\ndef g():\"\n",
    "\n",
    "def formatting_prompts_func(example,prompt=prompt):\n",
    "    output_texts = []\n",
    "    text = f\"{prompt}\\nPuzzle 4:\\n {example['program_str'][i]}\"\n",
    "    output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"Puzzle 4:\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "training_arguments=TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # evaluation_strategy=\"epoch\", #\"steps\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    # warmup_steps=2,\n",
    "    warmup_ratio=0.2,\n",
    "    max_steps = 12,\n",
    "    weight_decay=0.001,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"adamw_hf\",#\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    # group_by_length=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,#\"EleutherAI/gpt-neo-125m\",\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "output_dir = \"/media/data/flowers/OpenELM/models/codegen-QDAIF-smart\"\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "out=[15,17,16.5,16.,16.,17,16.25,17.3,15]\n",
    "len(out)\n",
    "out.sort(reverse=True)\n",
    "np.mean(out[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"/media/data/flowers/OpenELM/preprocess_p3.json\" \n",
    "\n",
    "# path = \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # QDAIF smart\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "f=data[0][\"program_str\"].split(\"def g\")[0]\n",
    "prompt=\"\"\n",
    "n_fewshot=3\n",
    "for i in range(1,1+n_fewshot):\n",
    "    prompt+=f\"Puzzle {i}:\\n\"+data[i][\"program_str\"]+\"\\n\\n\"\n",
    "    # prompt+=data[i][\"program_str\"]+\"\\n\"\n",
    "tag_response = f\"Puzzle {n_fewshot+1}:\\n\"\n",
    "tag_eo_completion = f\"Puzzle\"\n",
    "text = prompt+ f\"Puzzle {n_fewshot+1}:\\n\"+ f #+\"def g():\"\n",
    "# text = f#prompt+ f \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=256,do_sample=True,temperature=1.)\n",
    "    gen_text= tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text.split(tag_response)[1].split(tag_eo_completion)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text]*5\n",
    "encoding = tokenizer(texts, padding=True, return_tensors='pt').to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**encoding,max_new_tokens=256,do_sample=True,temperature=1.)\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_text = [puzz.split(tag_response)[1].split(tag_eo_completion)[0].strip() for puzz in generated_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(puzz+\"\\n\") for puzz in filter_text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/media/data/flowers/OpenELM/models\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "peft=True\n",
    "model_id = \"Salesforce/codegen-350M-mono\"#\"bigcode/starcoderbase-1b\"#\"Salesforce/codegen-350M-mono\" # mono is better\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"#torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if peft:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\",trust_remote_code=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\",trust_remote_code=True)\n",
    "    \n",
    "if peft:\n",
    "    # model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    def print_trainable_parameters(model):\n",
    "        \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        )\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16, \n",
    "        lora_alpha=32, \n",
    "        target_modules=[\"qkv_proj\"],# \"lm_head\" s[\"c_attn\",\"c_proj\"],# codegen [\"qkv_proj\"],#[\"query_key_value\"],  \n",
    "        lora_dropout=0.01, \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # layers_to_transform=[i for i in range(5,15)]\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "from datasets import load_dataset\n",
    "# train on puzzle generated (except P3 trainset)\n",
    "\n",
    "path_train= \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # put path here\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=path_train)\n",
    "\n",
    "# testset which is in fact the trainset from P3 dataset\n",
    "path_test = \"/media/data/flowers/OpenELM/preprocess_p3.json\"\n",
    "\n",
    " \n",
    "# /!\\ need to remove that, just for testing purpose /!\\\n",
    "\n",
    "path_train = path_test \n",
    "# ================================================\n",
    "\n",
    "test_set = load_dataset(\"json\", data_files=path_test)\n",
    "\n",
    "def filter_correct_puzz(example):\n",
    "    return (\n",
    "        example[\"fitness\"] >0.\n",
    "    )\n",
    "# only keep correct puzzles\n",
    "correct_dataset = squad_it_dataset.filter(filter_correct_puzz)\n",
    "\n",
    "tokenized_datasets = correct_dataset.map(lambda samples: tokenizer(samples[\"program_str\"]), batched=True)\n",
    "tok_test_set = test_set.map(lambda samples: tokenizer(samples[\"program_str\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tok_test_set,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        evaluation_strategy=\"epoch\", #\"steps\",\n",
    "        gradient_accumulation_steps=2,\n",
    "        # warmup_steps=2,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs = 2,\n",
    "        # max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        max_grad_norm=0.3,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = \"/media/data/flowers/OpenELM/models/codegen-QDAIF-smart\"\n",
    "trainer.model.save_pretrained(path_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, path_model)\n",
    "model = model.merge_and_unload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "path = \"/media/data/flowers/OpenELM/preprocess_p3.json\" \n",
    "\n",
    "# path = \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # QDAIF smart\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "f=data[0][\"program_str\"].split(\"def g\")[0]\n",
    "prompt=\"\"\n",
    "n_fewshot=2\n",
    "for i in range(1,n_fewshot):\n",
    "    prompt+=f\"Puzzle {i}:\\n\"+data[i][\"program_str\"]+\"\\n\\n\"\n",
    "text = prompt+ f\"Puzzle {n_fewshot}:\\n\"+ f +\"\\ndef g():\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(text)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"/media/data/flowers/OpenELM/preprocess_p3.json\" \n",
    "\n",
    "# path = \"/media/data/flowers/OpenELM/logs/elm/23-08-22_12:36/step_9/save_all.json\" # QDAIF smart\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "f=data[11][\"program_str\"].split(\"def g\")[0]\n",
    "prompt=\"\"\n",
    "n_fewshot=1\n",
    "for i in range(n_fewshot):\n",
    "    prompt+=f\"Puzzle {i}:\\n\"+data[i][\"program_str\"]+\"\\n\\n\"\n",
    "text = prompt+ f\"Puzzle {n_fewshot}:\\n\"+ f #+\"def g():\"\n",
    "# text = f\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=256)\n",
    "\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Salesforce/codegen2-1B\"#\"bigcode/starcoderbase-1b\"#\"Salesforce/codegen-350M-mono\" # mono is better\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\",trust_remote_code=True)\n",
    "f=data[11][\"program_str\"].split(\"def g\")[0]\n",
    "prompt=\"\"\n",
    "n_fewshot=2\n",
    "for i in range(n_fewshot):\n",
    "    prompt+=f\"Puzzle {i}:\\n\"+data[i][\"program_str\"]+\"\\n\\n\"\n",
    "text = prompt+ f\"Puzzle {n_fewshot}:\\n\"+ f #+\"def g():\"\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"], max_new_tokens=256)\n",
    "\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codegpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
