{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def load_maps(snapshot_path):\n",
    "    if \"json\" in snapshot_path:\n",
    "        file_mode = \"json\"\n",
    "    elif \"pkl\" in snapshot_path:\n",
    "        file_mode =  \"pkl\"\n",
    "    else:\n",
    "        print(\"not found\")\n",
    "        if snapshot_path[-1]==\"/\":\n",
    "            snapshot_path += \"maps.pkl\"\n",
    "        else:\n",
    "            snapshot_path += \"/maps.pkl\"\n",
    "        file_mode = \"pkl\"\n",
    "\n",
    "    if file_mode == \"json\":\n",
    "        with open(snapshot_path, \"r\") as f:\n",
    "            genomes = json.load(f)\n",
    "    else:\n",
    "        with open(snapshot_path, \"rb\") as f:\n",
    "            maps = pickle.load(f)\n",
    "        fitnesses = maps[\"fitnesses\"]\n",
    "        genomes = maps[\"genomes\"]\n",
    "        non_zeros = maps[\"nonzero\"]\n",
    "        genomes=[i.__dict__ for i in genomes.archive]\n",
    "    return genomes\n",
    "\n",
    "\n",
    "path = \"/home/flowers/work/OpenELM/analysis_P3/quality/to_analyse/maps_1_imgep_smart.json\"\n",
    "genomes = load_maps(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zeros={}\n",
    "indices_gen = [i for i, x in enumerate(genomes)]\n",
    "for i in indices_gen:\n",
    "    emb = str(genomes[i][\"emb\"])\n",
    "    if emb in non_zeros:\n",
    "        non_zeros[emb].append(i)\n",
    "    else:\n",
    "        non_zeros[emb] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "===========\n",
      "\n",
      "\n",
      "number of correct completion 0 / 5 trials\n",
      "idx 35\n",
      "The puzzle involves defining a function f that checks if a given subset of numbers is a subset of the Fibonacci sequence up to a certain limit. The solution function g generates the Fibonacci sequence up to the specified limit and returns it. The puzzle requires f(g()) to return True.\n",
      "\n",
      "def f(subset: list, limit=100) -> bool:\n",
      "    fibonacci = [0, 1]\n",
      "    while fibonacci[-1] + fibonacci[-2] <= limit:\n",
      "        fibonacci.append(fibonacci[-1] + fibonacci[-2])\n",
      "    return set(subset).issubset(fibonacci)\n",
      "\n",
      "def g(limit=100):\n",
      "    fibonacci = [0, 1]\n",
      "    while fibonacci[-1] + fibonacci[-2] <= limit:\n",
      "        fibonacci.append(fibonacci[-1] + fibonacci[-2])\n",
      "    return fibonacci\n",
      "\n",
      "assert f(g()) == True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_puzzle=1\n",
    "list_puzzle_2_eval={}\n",
    "count=0\n",
    "for emb,list_idx in non_zeros.items():\n",
    "    len_list_puz = len(list_idx)\n",
    "    if len_list_puz>=n_puzzle:\n",
    "        list_puzzle_2_eval[emb]=np.random.choice(list_idx, min(n_puzzle,6), replace=False)\n",
    "idx = np.random.choice(len(list_puzzle_2_eval), replace=False)\n",
    "value = list(list_puzzle_2_eval.values())[idx]\n",
    "key = list(list_puzzle_2_eval.keys())[idx]\n",
    "print(idx)\n",
    "for idx in value:\n",
    "    print(\"===========\\n\\n\")\n",
    "    print(f\"number of correct completion {genomes[idx]['n_correct']} / 5 trials\")\n",
    "    print(f\"idx {genomes[idx]['idx_generation']}\")\n",
    "    print(genomes[idx][\"description\"][0])\n",
    "\n",
    "    print(genomes[idx][\"program_str\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4163533367.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "```\n",
    "number of correct completion 2 / 5 trials\n",
    "idx 84\n",
    "The puzzle involves defining a function f that checks if a given string is a palindrome after removing non-alphanumeric characters, and a function g that generates a random palindrome string. The assert statement checks if function f returns True when applied to the output of function g.\n",
    "\n",
    "def f(string: str) -> bool:\n",
    "    cleaned_string = ''.join(char.lower() for char in string if char.isalnum())\n",
    "    return cleaned_string == cleaned_string[::-1]\n",
    "\n",
    "def g():\n",
    "    import random\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    palindrome = ''.join(random.choice(letters) for _ in range(7))\n",
    "    return palindrome + palindrome[::-1]\n",
    "\n",
    "assert f(g()) == True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3142497506.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    number of correct completion 0 / 5 trials\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "number of correct completion 0 / 5 trials\n",
    "idx 35\n",
    "The puzzle involves defining a function f that checks if a given subset of numbers is a subset of the Fibonacci sequence up to a certain limit. The solution function g generates the Fibonacci sequence up to the specified limit and returns it. The puzzle requires f(g()) to return True.\n",
    "\n",
    "def f(subset: list, limit=100) -> bool:\n",
    "    fibonacci = [0, 1]\n",
    "    while fibonacci[-1] + fibonacci[-2] <= limit:\n",
    "        fibonacci.append(fibonacci[-1] + fibonacci[-2])\n",
    "    return set(subset).issubset(fibonacci)\n",
    "\n",
    "def g(limit=100):\n",
    "    fibonacci = [0, 1]\n",
    "    while fibonacci[-1] + fibonacci[-2] <= limit:\n",
    "        fibonacci.append(fibonacci[-1] + fibonacci[-2])\n",
    "    return fibonacci\n",
    "\n",
    "assert f(g()) == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fitness', 'program_str', 'emb', 'explanation_emb', 'idx_generation', 'target_skills', 'puzzle_history', 'problem_func', 'solution_func', 'quality', 'description', 'is_valid', 'is_valid_explanation', 'pass_5', 'n_sample', 'n_correct'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genomes[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puz_list=[#n_correct 1 /5\n",
    "\"\"\"\n",
    "Puzzle description: The puzzle requires the student to define a function that generates a list of numbers based on given parameters, and then check if the generated list satisfies certain conditions.\n",
    "from typing import List \n",
    "def f(x: List[int], a=43536, s=3795, e=417606) -> bool:\n",
    "    return x[0] == a and x[-1] <= e and (x[-1] + s > e) and all([x[i] + s == x[i + 1] for i in range(len(x) - 1)])\n",
    "def g(a = 43536, s = 3795, e = 417606):\n",
    "    return list(range(a, e + 1, s))\n",
    "assert f(g()) == True\n",
    "\"\"\",\n",
    "#n_correct 3 /5\n",
    "\n",
    "\"\"\"\n",
    "The puzzle involves finding a missing number in a list of integers by calculating the sum of the list and comparing it to the expected sum of a sequence of numbers.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def f(missing_num: int, nums=[1, 2, 3, 4, 6, 7, 8, 9]) -> bool:\n",
    "    n = len(nums) + 1\n",
    "    total_sum = n * (n + 1) // 2\n",
    "    return missing_num == total_sum - sum(nums)\n",
    "\n",
    "def g(nums=[1, 2, 3, 4, 6, 7, 8, 9]) -> int:\n",
    "    n = len(nums) + 1\n",
    "    total_sum = n * (n + 1) // 2\n",
    "    return total_sum - sum(nums)\n",
    "\n",
    "assert f(g()) == True\n",
    "\"\"\",\n",
    "#difficulty 3 /5\n",
    "\n",
    "\"\"\"\n",
    "The puzzle requires the student to create a string expression using only the characters '1', '8', '-', '+', '*', and '/' that evaluates to 63 and contains three occurrences of the digit '8' and one occurrence of the digit '1'.\n",
    "def f(s: str) -> bool:\n",
    "    return set(s) <= set('18-+*/') and s.count('8') == 3 and (s.count('1') == 1) and (eval(s) == 63)\n",
    "def g():\n",
    "    return \"8*8-1**8\"\n",
    "assert f(g()) == True\n",
    "\"\"\"\n",
    "          \n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# This is a pretraining â€¦. datapoint.\n",
    "# ###\n",
    "# Does the previous paragraph demarcated within ### and ###\n",
    "# contain informative signal for pre-training a large-language model?\n",
    "# An informative datapoint should be well-formatted, contain some\n",
    "# usable knowledge of the world, and strictly NOT have any harmful,\n",
    "# racist, sexist, etc. content.\n",
    "# OPTIONS:\n",
    "# - yes\n",
    "# - no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fine_finetuning=\"\"\"###\n",
    "This is a pretraining datapoint:\n",
    "```python\n",
    "from typing import List\n",
    "from itertools import permutations\n",
    "\n",
    "def f(nums: List[int]) -> bool:\n",
    "    def check_condition(perm):\n",
    "        # Define your condition here\n",
    "        return True\n",
    "\n",
    "    for perm in permutations(nums):\n",
    "        if check_condition(perm):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def g():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "assert f(g()) == True\n",
    "```\n",
    "###\n",
    "Does the previous paragraph demarcated within ### and ###\n",
    "contain informative signal for fine-tuning a large-language model?\n",
    "An informative datapoint should be well-formatted, contain some\n",
    "usable knowledge of the world.\n",
    "OPTIONS:\n",
    "- Yes\n",
    "- No\n",
    "\"\"\"\n",
    "\n",
    "prompt_education=\"\"\"###\n",
    "This is a educational datapoint to give to students during their exams:\n",
    "```python\n",
    "from typing import List\n",
    "from itertools import permutations\n",
    "\n",
    "def f(nums: List[int]) -> bool:\n",
    "    def check_condition(perm):\n",
    "        # Define your condition here\n",
    "        return True\n",
    "\n",
    "    for perm in permutations(nums):\n",
    "        if check_condition(perm):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def g():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "assert f(g()) == True\n",
    "```\n",
    "###\n",
    "Does the previous paragraph demarcated within ### and ###\n",
    "contain informative signal to a large-language model?\n",
    "An informative datapoint should be well-formatted, contain some\n",
    "usable knowledge of the world.\n",
    "OPTIONS:\n",
    "- Yes\n",
    "- NO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_logprobs\n",
    "import sys \n",
    "sys.path.append(\"/home/flowers/work/OpenELM\")\n",
    "from openai import OpenAI\n",
    "from key import OPENAI_API_KEY\n",
    "from utils_openai import get_completion\n",
    "cfg: dict = {\n",
    "\"temperature\": 0.,\n",
    "# \"top_p\": 1.,\n",
    "# TODO: rename config option?\n",
    "\"model\": \"gpt-3.5-turbo-0125\",\n",
    "\"logprobs\": True,\n",
    "\"top_logprobs\": 5,\n",
    "}\n",
    "max_retries=10\n",
    "timeout=30\n",
    "client = OpenAI(api_key=OPENAI_API_KEY,max_retries=max_retries, timeout=timeout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "logits_yes = encoding.encode(\"yes\")\n",
    "logits_no = encoding.encode(\"no\")\n",
    "\n",
    "logits_yes, logits_no\n",
    "logit_bias={str(logits_yes[0]): +100, str(logits_no[0]):+100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m out\u001b[38;5;241m=\u001b[39mget_completion(client,prompt, cfg,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# out.choices[0].logprobs.content[0].top_logprobs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlogprobs\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtop_logprobs:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mtoken, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, logprobs = \u001b[39m\u001b[38;5;124m\"\u001b[39m ,np\u001b[38;5;241m.\u001b[39mexp(tok\u001b[38;5;241m.\u001b[39mlogprob))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "out=get_completion(client,prompt_fine_finetuning, cfg,temperature=0.0)\n",
    "# out.choices[0].logprobs.content[0].top_logprobs\n",
    "\n",
    "for tok in out.choices[0].logprobs.content[0].top_logprobs:\n",
    "    print(tok.token, \", logprobs = \" ,np.exp(tok.logprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###\\nThis is a pretraining datapoint:\\n```python\\nfrom typing import List\\nfrom itertools import permutations\\n\\ndef f(nums: List[int]) -> bool:\\n    def check_condition(perm):\\n        # Define your condition here\\n        return True\\n\\n    for perm in permutations(nums):\\n        if check_condition(perm):\\n            return True\\n    return False\\n\\ndef g():\\n    return [1, 2, 3]\\n\\nassert f(g()) == True\\n```\\n###\\nDoes the previous paragraph demarcated within ### and ###\\ncontain informative signal for fine-tuning a large-language model?\\nAn informative datapoint should be well-formatted, contain some\\nusable knowledge of the world.\\nOPTIONS:\\n- yes\\n- no\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 1.5)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $0.0005 / 1K tokens\t$0.0015 / 1K tokens\n",
    "0.0005*1000, 0.0015*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes , logprobs =  0.7980235505503027\n",
      "no , logprobs =  0.08322585684915011\n",
      "- , logprobs =  0.08321569559238617\n",
      "Yes , logprobs =  0.030966168783352257\n",
      "No , logprobs =  0.0028206991478242705\n",
      "=====\n",
      "\n",
      "yes , logprobs =  0.955584274327124\n",
      "no , logprobs =  0.02049601504053926\n",
      "- , logprobs =  0.01635460123711706\n",
      "Yes , logprobs =  0.007025766214712721\n",
      "No , logprobs =  0.00030244473239863503\n",
      "=====\n",
      "\n",
      "yes , logprobs =  0.7980235505503027\n",
      "no , logprobs =  0.08322585684915011\n",
      "- , logprobs =  0.08321569559238617\n",
      "Yes , logprobs =  0.030966168783352257\n",
      "No , logprobs =  0.0028206991478242705\n",
      "=====\n",
      "\n",
      "yes , logprobs =  0.8623405126885293\n",
      "- , logprobs =  0.045371972971242534\n",
      "no , logprobs =  0.044624943722659634\n",
      "Yes , logprobs =  0.044412069984045864\n",
      "No , logprobs =  0.0026025670346923966\n",
      "=====\n",
      "\n",
      "yes , logprobs =  0.8623405126885293\n",
      "- , logprobs =  0.045371972971242534\n",
      "no , logprobs =  0.044624943722659634\n",
      "Yes , logprobs =  0.044412069984045864\n",
      "No , logprobs =  0.0026025670346923966\n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI programming assistant\"},#You are a coding assistant, skilled in writting code with creative flair.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_fine_finetuning}\n",
    "    ],temperature=None,logprobs=True,top_logprobs=5,model = \"gpt-3.5-turbo-0125\",max_tokens=1\n",
    "    )\n",
    "\n",
    "    for tok in completion.choices[0].logprobs.content[0].top_logprobs:\n",
    "        print(tok.token, \", logprobs = \" ,np.exp(tok.logprob))\n",
    "    print(\"=====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "path_model= \"/home/flowers/work/hf/deepseek-coder-1.3b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_model,local_files_only=True)#LlamaTokenizer.from_pretrained(model_id,local_files_only=True)\n",
    "dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_model,\n",
    "\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    ")\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "soft=torch.nn.Softmax(dim=1)\n",
    "\n",
    "prompt_model=\"\"\"You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = prompt_model.format(instruction = prompt_education)# prompt_fine_finetuning)\n",
    "yes_prompt= full_prompt# + \"Yes\"\n",
    "# no_prompt= full_prompt + \"No\"\n",
    "# print(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    input_tokenise=tokenizer(yes_prompt, return_tensors='pt').to(\"cuda\")\n",
    "    out_yes=model(**input_tokenise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670896768569946"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return p(yes|prompt)\n",
    "k=5\n",
    "yes_logits=soft(out_yes.logits[:,-1]).cpu().detach() #logits associated with the token \"yes\"\n",
    "values,indices=torch.topk(yes_logits, k)\n",
    "list_token=tokenizer.batch_decode(indices.T)\n",
    "values,list_token\n",
    "if \"Yes\" in list_token:\n",
    "    idx = list_token.index(\"Yes\")\n",
    "elif \"yes\" in list_token:\n",
    "    idx = list_token.index(\"yes\")\n",
    "elif \"No\" in list_token:\n",
    "    idx = list_token.index(\"No\")\n",
    "elif \"no\" in list_token:\n",
    "    idx = list_token.index(\"no\")\n",
    "# else:\n",
    "    # return 0.5\n",
    "values[[0],idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(torch.argmax(yes_logits, dim=1)),yes_logits[0,torch.argmax(yes_logits, dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f49aa70c370>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(yes_logits[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logits bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes , logprobs =  0.825312662166235\n",
      "no , logprobs =  0.17468727563723466\n",
      "Yes , logprobs =  3.720075976020836e-44\n",
      "- , logprobs =  3.720075976020836e-44\n",
      "No , logprobs =  3.720075976020836e-44\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI programming assistant\"},#You are a coding assistant, skilled in writting code with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "],temperature=0.0,logprobs=True,top_logprobs=5,model = \"gpt-3.5-turbo-0125\",max_tokens=1,logit_bias=logit_bias,\n",
    ")\n",
    "\n",
    "for tok in completion.choices[0].logprobs.content[0].top_logprobs:\n",
    "    print(tok.token, \", logprobs = \" ,np.exp(tok.logprob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
